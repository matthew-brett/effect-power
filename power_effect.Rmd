---
title: "Power and effect size"
output: html_notebook
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by
placing your cursor inside it and pressing *Cmd+Shift+Enter*.

## The situation

I have a new cognitive treatment for depression.

I randomize 60 patients into two groups of 30.  The first groups gets normal
clinical care.  The second gets my treatment.

I measure their levels of depression on the [PHQ-9
questionnaire](https://en.wikipedia.org/wiki/PHQ-9)

Here are the 30 values in each group.

```{r}
data <- read.csv('phq9_data.csv')
data
```
Let's investigate with some histograms:

```{r}
hist(data$controls, breaks=10)
```

```{r}
hist(data$treated, breaks=10)
```

Side by side:

```{r}
# Two plots side by side.
par(mfrow=c(1,2))
hist(data$controls, breaks=10)
hist(data$treated, breaks=10)
```

## Some more information

```{r}
summary(data)
```

## Questions 1

Do you think the treatment was effective?

Why?  Or why not?  How sure are you?  How effective is it?  What other
information would you like to see to be more sure of your answers?

You can ask me to calculate anything you like...

## Another trial

I made a rather slight modification to my treatment program, and ran another
trial, to get the similar data.

```{r}
data2 <- read.csv('phq9_data2.csv')
data2
```

```{r}
# Two plots side by side.
par(mfrow=c(1,2))
hist(data2$controls2)
hist(data2$treated2)
```

```{r}
summary(data2)
```

```{r}
t.test(data2$controls2, data2$treated2)
```

## Questions 2

Do you think treatment2 was effective?  Is it more effective than the first treatment?

Why?  Or why not?  How sure are you?

## Will I find anything?

Now imagine I'm doing the trial again.

I can only afford 20 patients per group this time (funding is tight).

The treatment is relatively labour intensive, so I'm only interested to continue
studying this treatment if it causes a 3 point drop on the PHQ9 scale.

But I know, of course, that if I take some random sample of treated patients,
when the true population effect is 3, then my mean effect in the sample will be
a bit random.

## Random sampling

Let's imagine that the PHQ9 scores are roughly normally distributed, with a
mean of 10 (now), and a standard deviation of 4.  Here's a normal distribution
with mean 15 and standard deviation 4.

```{r}
control_population <- rnorm(10000, 15, 4)
hist(control_population)
```

If I take sample of 10 patients from this population, it might look like this:

```{r}
control_sample1 <- sample(control_population, 20)
hist(control_sample1, breaks=10)
```

Or this:

```{r}
control_sample2 <- sample(control_population, 20)
hist(control_sample2, breaks=10)
```

Each sample will have a slightly different mean:

```{r}
mean(control_sample1)
mean(control_sample2)
```

Now let's imagine my treatment has worked just as I wanted, and there is a 3
point drop in the PHQ9 score.  My new population looks like this:

```{r}
treated_population <- rnorm(10000, 12, 4)
hist(treated_population)
```
```{r}
mean(control_population) - mean(treated_population)
```

Of course random samples from this population also differ one from another:

```{r}
treated_sample1 <- sample(treated_population, 20)
treated_sample2 <- sample(treated_population, 20)
par(mfrow=c(1, 2))
hist(treated_sample1, breaks=10)
hist(treated_sample2, breaks=10)
```

Again, the means will differ:

```{r}
mean(treated_sample1)
mean(treated_sample2)
```

Now consider my experiment, where I'm looking for a 3 point drop.   Let's
imagine there really is a 3 point drop, as there is in the `treated_population`
above.

I'm sampling my first (control) group from the `control_population` - like this:

```{r}
controls <- sample(control_population, 20)
```

I'm sampling my second (treated) group from the `treated_population` - like
this:

```{r}
treated <- sample(treated_population, 20)
```

I can look for a difference in means:

```{r}
mean_diff <- mean(controls) - mean(treated)
mean_diff
```

Notice that the difference in means is not exactly 3 - because the two samples
were random, so their means are random, and so is the difference between means.

Or do a t-test:

```{r}
t.test(controls, treated)
```

## Questions 3

Is 20 patients enough, in each group?  How will I decide if I have an effect?
How likely am I to find the effect I am interested in?

## False positives

Our t-test is designed to protect us from *false positives*.

A *false positive* is where we find a significant test result, but there is in
fact no underlying difference in the population means.

In our case, imagine that the treatment was completely ineffective.  In that case the control sample and the treatment sample would in fact come from the same population, of patients who had (effectively) had no treatment.

The t-test helps us avoid concluding that there is a difference, in this case,
where there is, in fact, no difference.  It helps us avoid false positives.

Let us show the t-test at work.  This code below just compares two samples
that are, in fact, both drawn from the same control distribution, and
therefore, where there is no underlying difference in the population means.

Usually we say we may have found an effect when the statistical test is
significant.

Remember that *significant* is not, on its own, very meaningful.  A
significant test could still be a false positive.

So, in our situation, what is the chance that the - say - t-test will be
significant, if there really is no change in the *population* PHQ9 scores.

We can do lots of simulations of our proposed experiment, to find out.

```{r}
# Testing for differences, when there is no population difference.
mean_differences_null <- numeric(10000)
p_values_null <- numeric(10000)
for (i in 1:10000) {  # Repeat stuff between {} 10000 times.
  # You've seen this above. We're just repeating it 10000 times.
  controls <- sample(control_population, 20)  # Sample from the control group.
  treated <- sample(control_population, 20)  # Another from the control group.
  # What is the difference between the sample means?
  mean_diff <- mean(controls) - mean(treated)
  # What does the t-test say?
  test_result <- t.test(controls, treated)
  # Store the results from this trial
  # Store the mean difference.
  mean_differences_null[i] <- mean_diff
  # Store the t-test p value.
  p_values_null[i] <- test_result$p.value
  # The } next says "go back to the { above, and repeat".
}
```

Here are the mean and standard deviation of the mean differences:

```{r}
mean(mean_differences_null)
sd(mean_differences_null)
```

Notice the mean of the mean differences is near 0, as it should be because, on
average, two samples from the same population should have around the same
mean.

```{r}
hist(mean_differences_null)
```

Some of these mean differences did not come out as significant on the t-test.

```{r}
# Select just the mean differences where the p values was < 0.05
significant_diffs_null <- mean_differences_null[p_values_null < 0.05]
# How many of the differences were significant?
n_sig_null <- length(significant_diffs_null)
n_sig_null
```

What proportion of these differences gave a significant result?

```{r}
n_sig_null / 10000
```

## Question

Why are 5% of the tests here significant, when we know that there was in fact no difference between the populations (because they were the same)?

## Power

Now consider the case where the treatment does in fact cause an average 3 point drop in the PHQ9 scale.  That is, if we could get the population of all patients treated with this condition, we would find that their PHQ9 values had dropped, on average, by 3 points.

Power is the chance that I will identify an effect.  In this case, it is the chance I will identify this effect - of the 3 point drop.

As we saw, usually we are only happy to say we have found an effect when the
statistical test is significant.

So, in our situation, what is the chance that the - say - t-test will be
significant, if there *really is* a drop of 3 in the PHQ9 scale, in the treated population?

We can do more simulations of our proposed experiment, to find out.  We sample from the control population (as before), but we also sample from the treated population, with the average 3 point drop.

```{r}
mean_differences_3 <- numeric(10000)
p_values_3 <- numeric(10000)
for (i in 1:10000) {  # Repeat stuff between {} 10000 times.
  # You've seen this above. We're just repeating it 10000 times.
  controls <- sample(control_population, 20)
  treated <- sample(treated_population, 20)
  mean_diff <- mean(controls) - mean(treated)
  test_result <- t.test(controls, treated)
  # Store the results from this trial
  mean_differences_3[i] <- mean_diff
  p_values_3[i] <- test_result$p.value
}
length(mean_differences_3)
```

Show the distribution of mean differences for this case:

```{r}
hist(mean_differences_3)
```

Here are the mean and standard deviation of the mean differences:

```{r}
mean(mean_differences_3)
sd(mean_differences_3)
```

Some of these mean differences did not come out as significant on the t-test.

```{r}
# Select just the significant differences
significant_diffs_3 <- mean_differences_3[p_values_3 < 0.05]
length(significant_diffs_3)
```

What proportion of the tests were significant?

```{r}
# 10000 is the number of simulations.
p_sig_3 <- length(significant_diffs_3) / 10000 
p_sig_3
```

Show all differences, and the significant differences, on the same plot.

```{r}
# Fancy plotting code.   Don't worry about this chunk.
md_hist <- hist(mean_differences_3, plot=FALSE)
p_hist <- hist(significant_diffs_3, breaks=md_hist$breaks, plot=FALSE)
# Make some color definitions for the plots
lt_blue <- rgb(173, 216, 230, max=255, alpha=80, names="lt.blue")
lt_pink <- rgb(255, 192, 203, max=255, alpha=80, names="lt.pink")
# All mean differences (significant or not).
plot(md_hist, col=lt_blue, main='All and significant mean differences')
# Just the differences where the p-value is "significant".
plot(p_hist, col=lt_pink, add=TRUE)
```

So, if we accept a p value of 0.05 as significant we have the following chance
of getting a "significant" result, if the effect really is 3.

```{r}
p_sig_3
```

Which means we have the following chance of *missing* the effect (because our test was not significant).

```{r}
# Chances that we will miss an effect when it is there in the population.
beta_3 <- 1 - p_sig_3
beta_3
```

## Questions

Is it OK to have a roughly 35% chance of failing to get a significant result?

What kind of percent chance of missing the result are you prepared to accept?

What are the consequences of missing the result?

What could we do to improve our chances of getting a significant result?

What would cause us to get lower chances of a significant result?

## Sample size

Now let's say I can only afford 10 patients in each sample.

What effect do you think this will have on our chances of getting a significant
result, when there is an effect of 3 points, as above?

```{r}
mean_differences_10 <- numeric(10000)
p_values_10 <- numeric(10000)
for (i in 1:10000) {  # Repeat stuff between {} 10000 times.
  # You've seen this above. We're just repeating it 10000 times.
  controls <- sample(control_population, 10)  # Just 10 patients.
  treated <- sample(treated_population, 10)  # Just 10 patients
  mean_diff <- mean(controls) - mean(treated)
  test_result <- t.test(controls, treated)
  # Store the results from this trial
  mean_differences_10[i] <- mean_diff
  p_values_10[i] <- test_result$p.value
}
```

Notice the spread:

```{r}
hist(mean_differences_10)
```

The standard deviation reflects the spread:

```{r}
mean(mean_differences_10)
sd(mean_differences_10)
```

Fewer results are significant:

```{r}
significant_diffs_10 <- mean_differences_10[p_values_10 < 0.05]
p_sig_10 <- length(significant_diffs_10) / 10000
p_sig_10
```

Showing all mean differences with those that correspond to a significant t-test:

```{r}
# Plotting again.
md_hist <- hist(mean_differences_10, plot=FALSE)
p_hist <- hist(significant_diffs_10, breaks=md_hist$breaks, plot=FALSE)
plot(md_hist, col=lt_blue, main='All and significant mean differences; n=10')
plot(p_hist, col=lt_pink, add=TRUE)
```

Now you have a:

```{r}
# Chance of missing
beta_n10 <- 1 - p_sig_10
beta_n10
```

chance of missing a 3 point difference, if you use the 0.05 criterion to
identify tests as positive.  Is that acceptable?  What consequences could
arise?

## Effect size

Let us return to a single experiment, using our simulated data.

```{r}
controls <- sample(control_population, 20)
treated <- sample(treated_population, 20)
mean_diff <- mean(controls) - mean(treated)
mean_diff
```

This difference is our best estimate, from the experiment, of the difference
between the control *population* mean (which we know, in fact, was 15), and
the *treated population* mean (which was, in fact 12).  Yes, it's a little
off, because it is somewhat random, but it's the best we can do with the data
we have.

If we look at all the mean of all the mean differences we found in all 10000
experiments, we see that it's very close to the right answer, of 3.

```{r}
mean(mean_differences_3)
```

But now consider the mean differences that we get if we only look at the
experiments where the t-test turned out to be significant:

```{r}
# Mean of the differences, restricted to the experiments where the t-test was
# significant.
mean(significant_diffs_3)
```

## Have I found something?  Or was this an accident?

Why is this estimate different from the one above?  And why is it so far off?

Now imagine that you only report results when the t-test is significant.  You
also report the *effect size*, which is the mean difference.  Why might this
be misleading?

Let us say that I have done a t-test between two samples of 30 patients, call
these sample A and B.  I have a p-value of 0.05.   I know that sample A was
from the control population, but I don't know if sample B was from a
population where the treatment does not work (and so should have the same
underlying population as the control) or from a treatment that caused an
average 3 point drop in the score.

## Questions

Given this t test value, do you think my sample B was from the population
where the treatment worked, or where it didn't work?

How sure are you?

## A COVID test

Let's say you've had a COVID-19 test as part of a screening, to get into a
crowded indoor event.

Your test is positive.

The population incidence of COVID-19 is 0.01 or 1%.

Assume the test has a false negative rate of 0.1 (10%) - of 100 people who do
have COVID-19, 10% will get a negative test result from this test.

The test has an 0.08 positive rate (8%). This means that, of 100 people who do
*not* have COVID-19, 8% will have a positive test result from this test.

## Question

What are the chances that you actually have COVID-19?

In case this number is useful to you:

```{r}
0.99 * 0.08
```

## Back to the false positive / false negative question

Remember sample B and the significant t-test?

We give a name to the situation where sample B corresponds to an ineffective
treatment, so the population from which sample B was drawn has the exact same
mean score as the control sample.  Call this situation *B-* (B is negative).

Call the situation where sample B corresponds to a treatment causing an average
3 point drop in scores - *B+* (B is positive).

Call the situation where we have a significant statistical test *T+* (the test
is positive).

## Question

Returning to the question above - given that we have a positive statistical
test (situation T+), what is the chance that sample B really comes from an
effective treatment group (B+)?  What information do we need in order to
decide?
